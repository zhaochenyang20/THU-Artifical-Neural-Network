########################
# Missing Files
########################
# config.json

########################
# Additional Files
########################
# data
# pipeline.py
# output
# test_results
# tokenizer
# readme.md
# config_base.json

########################
# Filled Code
########################
# ../codes/model_tfmr.py:1
            #! 返回下三角矩阵
            torch.ones((1, 1, max_positions, max_positions), dtype=int).tril()

# ../codes/model_tfmr.py:2
        attn_weights = torch.matmul(query, key.transpose(-1, -2))
        query_size, key_size = query.shape[-2], key.shape[-2]
        causal_mask = self.bias[:, :, key_size - query_size : key_size, :key_size].to(
            torch.bool
        )
        attn_weights = torch.where(
            causal_mask, attn_weights, self.masked_bias.to(attn_weights.dtype)
        )
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_output = torch.matmul(attn_weights, value)

# ../codes/model_tfmr.py:3
        # return tensor.reshape(tensor.shape[:-1] + (num_heads, attn_head_size)).permute(
        #     0, 2, 1, 3
        # )
        # in_shape = tensor.size()
        # out_shape = in_shape[:-1] + (num_heads, attn_head_size)
        # tensor = tensor.view(out_shape)
        # tensor = tensor.permute(0, 2, 1, 3)
        in_shape = tensor.size()
        out_shape = in_shape[:-1] + (num_heads, attn_head_size)
        tensor = tensor.view(out_shape)
        tensor = tensor.permute(0, 2, 1, 3)
        return tensor

# ../codes/model_tfmr.py:4
        # tensor = tensor.permute(0, 2, 1, 3)
        # return tensor.reshape(tensor.shape[:-2] + (num_heads * attn_head_size,))
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        in_shape = tensor.size()
        out_shape = in_shape[:-2] + (num_heads * attn_head_size,)
        tensor = tensor.view(out_shape)
        return tensor

# ../codes/model_tfmr.py:5
        # hidden_states = residual + attn_output
        # residual = hidden_states

        residual = hidden_states = attn_output + residual
        hidden_states = residual + self.mlp(self.ln_2(hidden_states))

# ../codes/model_tfmr.py:6
        if past_key_values is not None:
            past_length = past_key_values[0][0].size(-2)

        else:
            past_length = 0
            past_key_values = tuple([None] * len(self.h))

        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
        position_embeds = self.wpe(position_ids)


# ../codes/model_tfmr.py:7
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            pad_pos = torch.eq(shift_labels, PAD_ID).to(torch.float).to(labels.device)
            pad_pos = torch.cat([torch.zeros([shift_labels.size()[0], 1]).to(labels.device), pad_pos[:, :-1]], 1)
            loss_mask = 1. - pad_pos
            loss = ce_loss_fct(shift_logits.view(-1, shift_logits.shape[-1]), shift_labels.view(-1))
            loss = torch.mean(torch.sum(loss.view(shift_labels.size()[0], -1) * loss_mask, 1) / (torch.sum(loss_mask, 1) + 1e-20))

# ../codes/model_tfmr.py:8
                        # Reference: https://github.com/huggingface/transformers/blob/5041bc3511d098814598cf1cfc6c6bd20e72c144/src/transformers/generation_logits_process.py
                        sorted_logits, sorted_indices = torch.sort(
                            logits, descending=True
                        )
                        cumulative_probs = torch.cumsum(
                            F.softmax(sorted_logits, dim=-1), dim=-1
                        )

                        # Remove tokens with cumulative probability above the threshold
                        sorted_indices_to_remove = cumulative_probs > top_p
                        # Shift the indices to the right to keep also the first token above the threshold
                        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[
                            :, :-1
                        ].clone()
                        sorted_indices_to_remove[:, 0] = 0
                        sorted_indices = (
                            sorted_indices
                            + torch.arange(
                                sorted_indices.shape[0], device=device, dtype=torch.long
                            ).unsqueeze(-1)
                            * sorted_indices.shape[1]
                        )
                        indices_to_remove = torch.masked_select(
                            sorted_indices, sorted_indices_to_remove
                        )

                        logits = logits.reshape(-1)
                        logits = torch.index_fill(
                            logits, 0, indices_to_remove, -float("inf")
                        )
                        logits = logits.reshape(
                            sorted_indices.shape[0], sorted_indices.shape[1]
                        )


# ../codes/model_tfmr.py:9
                        # Reference https://github.com/huggingface/transformers/blob/5041bc3511d098814598cf1cfc6c6bd20e72c144/src/transformers/generation_logits_process.py
                        indices_to_remove = logits < torch.topk(logits, top_k, dim=1)[
                            0
                        ][:, -1].unsqueeze(dim=-1)
                        logits = logits.masked_fill(indices_to_remove, -float("inf"))

# ../codes/main.py:1
            #! Warning
            tgt_ids = input_ids[:, 1:]
            input_ids = input_ids[:, :-1]
            lm_logits = model(input_ids)["logits"]
            pad_pos = torch.eq(tgt_ids, PAD_ID).to(torch.float).to(device)
            pad_pos = torch.cat([torch.zeros([ed-st, 1]).to(device), pad_pos[:, :-1]], 1)
            loss_mask = 1. - pad_pos
            loss = torch.sum(loss.view(input_ids.size()[0], -1) * loss_mask, 1) / (torch.sum(loss_mask, 1) + 1e-20)



########################
# References
########################
# https://github.com/huggingface/transformers/blob/5041bc3511d098814598cf1cfc6c6bd20e72c144/src/transformers/generation_logits_process.py

########################
# Other Modifications
########################
# _codes/model_tfmr.py -> ../codes/model_tfmr.py
# 1 - from turtle import position
# 14 +
# 30 +
# 38 +             #! Warning!
# 51 -
# 65 +         #! Warning
# 89 +         #! Warning
# 106 +         #! Warning
# 119 +         self, hidden_states, layer_past=None, use_cache=False,
# 90 -         self,
# 91 -         hidden_states,
# 92 -         layer_past=None,
# 93 -         use_cache=False,
# 177 +         self, hidden_states, layer_past=None, use_cache=False,
# 151 -         self,
# 152 -         hidden_states,
# 153 -         layer_past=None,
# 154 -         use_cache=False,
# 182 +             hidden_states, layer_past=layer_past, use_cache=use_cache,
# 159 -             hidden_states,
# 160 -             layer_past=layer_past,
# 161 -             use_cache=use_cache,
# 187 +         #! Warning
# 214 +         self.h = nn.ModuleList(
# 188 -         self.h = nn.ModuleList([TfmrBlock(config) for _ in range(config.num_hidden_layers)])
# 188 ?         ------ - ^^^^^^^^^^^^^^                                                            -
# 215 +             [TfmrBlock(config) for _ in range(config.num_hidden_layers)]
# 215 ?           ^^
# 216 +         )
# 223 +         self, input_ids, past_key_values=None, use_cache=None,
# 195 -         self,
# 196 -         input_ids,
# 197 -         past_key_values=None,
# 198 -         use_cache=None,
# 231 +         #! Warning
# 258 +             outputs = block(hidden_states, layer_past=layer_past, use_cache=use_cache,)
# 222 -             outputs = block(
# 223 -                 hidden_states,
# 224 -                 layer_past=layer_past,
# 225 -                 use_cache=use_cache,
# 226 -             )
# 232 -             all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)
# 232 ?                                                          --------------------------------
# 264 +             all_self_attentions = all_self_attentions + (
# 265 +                 outputs[2 if use_cache else 1],
# 266 +             )
# 290 +         self, input_ids, past_key_values=None, labels=None, use_cache=None, PAD_ID=None,
# 256 -         self,
# 257 -         input_ids,
# 258 -         past_key_values=None,
# 259 -         labels=None,
# 260 -         use_cache=None,
# 261 -         PAD_ID=None,
# 293 +             input_ids=input_ids, past_key_values=past_key_values, use_cache=use_cache,
# 264 -             input_ids=input_ids,
# 265 -             past_key_values=past_key_values,
# 266 -             use_cache=use_cache,
# 301 +             #! Warning
# 285 -          }
# 285 ? -
# 320 +         }
# 321 +
# 322 +     def inference(
# 286 -
# 323 +         self,
# 323 ?         +++++
# 287 -
# 288 -     def inference(self, device, PAD_ID, batch_size, maxlen, decode_strategy, temperature, top_p=1.0, top_k=50267):
# 324 +         device,
# 325 +         PAD_ID,
# 326 +         batch_size,
# 327 +         max_len,
# 328 +         decode_strategy,
# 329 +         temperature,
# 330 +         top_p=1.0,
# 331 +         top_k=50267,
# 332 +     ):
# 292 -             for i in range(0, int(5000/batch_size)+1):
# 336 +             for i in range(0, int(5000 / batch_size) + 1):
# 336 ?                                       + +           + +
# 293 -                 input_ids = torch.tensor([[PAD_ID] for _ in range(batch_size)]).to(device)
# 293 ?                                                                                    -------
# 337 +                 input_ids = torch.tensor([[PAD_ID] for _ in range(batch_size)]).to(
# 338 +                     device
# 339 +                 )
# 296 -                 for _ in range(maxlen):
# 342 +                 for _ in range(max_len):
# 342 ?                                   +
# 343 +                     outputs = self(
# 297 -                     outputs = self(input_ids, past_key_values=past_key_values, use_cache=True)
# 297 ?                     ------- - ^^^^^                                                          -
# 344 +                         input_ids, past_key_values=past_key_values, use_cache=True
# 344 ?                       ^^
# 345 +                     )
# 397 +
# 310 -                     prob = logits.softmax(dim=-1) # shape: (batch_size, num_vocabs)
# 398 +                     prob = logits.softmax(dim=-1)  # shape: (batch_size, num_vocabs)
# 398 ?                                                   +
# 311 -                     now_token = torch.multinomial(prob, 1)[:, :1] # shape: (batch_size)
# 399 +                     now_token = torch.multinomial(prob, 1)[:, :1]  # shape: (batch_size)
# 399 ?                                                                   +
# 323 -         self.train() # return to training mode
# 411 +         self.train()  # return to training mode
# 411 ?                     +
# 325 -
# _codes/tokenizer.py -> ../codes/tokenizer.py
# 7 +
# 19 -     bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¡"), ord("¬")+1))+list(range(ord("®"), ord("ÿ")+1))
# 20 +     bs = (
# 21 +         list(range(ord("!"), ord("~") + 1))
# 22 +         + list(range(ord("¡"), ord("¬") + 1))
# 23 +         + list(range(ord("®"), ord("ÿ") + 1))
# 24 +     )
# 22 -     for b in range(2**8):
# 27 +     for b in range(2 ** 8):
# 27 ?                     +  +
# 25 -             cs.append(2**8+n)
# 30 +             cs.append(2 ** 8 + n)
# 30 ?                        +  + + +
# 34 +
# 48 +
# 43 -     def __init__(self, encoder, bpe_merges, errors='replace'):
# 43 ?                                                    ^       ^
# 50 +     def __init__(self, encoder, bpe_merges, errors="replace"):
# 50 ?                                                    ^       ^
# 45 -         self.decoder = {v:k for k,v in self.encoder.items()}
# 52 +         self.decoder = {v: k for k, v in self.encoder.items()}
# 52 ?                           +        +
# 46 -         self.errors = errors # how to handle errors in decoding
# 53 +         self.errors = errors  # how to handle errors in decoding
# 53 ?                             +
# 48 -         self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}
# 55 +         self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
# 55 ?                                +
# 60 +         self.pat = re.compile(
# 53 -         self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
# 53 ?         -------- - ^^^^^^^^^^^                                                                                 -
# 61 +             r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
# 61 ?           ^^
# 62 +         )
# 65 -             bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
# 65 ?                                    - -                                            ^   ^
# 74 +             bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float("inf")))
# 74 ?                                                                                 ^   ^
# 80 -                 if word[i] == first and i < len(word)-1 and word[i+1] == second:
# 89 +                 if word[i] == first and i < len(word) - 1 and word[i + 1] == second:
# 89 ?                                                      + +            + +
# 81 -                     new_word.append(first+second)
# 90 +                     new_word.append(first + second)
# 90 ?                                          + +
# 92 -         word = ' '.join(word)
# 92 ?                ^ ^
# 101 +         word = " ".join(word)
# 101 ?                ^ ^
# 99 -             token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
# 99 ?                     ^^                                                 ^     ^
# 108 +             token = "".join(self.byte_encoder[b] for b in token.encode("utf-8"))
# 108 ?                     ^^                                                 ^     ^
# 100 -             bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(' '))
# 100 ?                                                                                ^ ^
# 109 +             bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(" "))
# 109 ?                                                                                ^ ^
# 106 -             token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
# 106 ?                     ^^                                                 ^     ^
# 115 +             token = "".join(self.byte_encoder[b] for b in token.encode("utf-8"))
# 115 ?                     ^^                                                 ^     ^
# 116 +             bpe_tokens.extend(
# 107 -             bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
# 107 ?             ^^^^^^^^^^^^^^^^^^                                                               ^ ^^
# 117 +                 self.encoder[bpe_token] for bpe_token in self.bpe(token).split(" ")
# 117 ?             ^^^^                                                               ^ ^
# 118 +             )
# 111 -         text = ''.join([self.decoder[token] for token in tokens])
# 111 ?                ^^
# 122 +         text = "".join([self.decoder[token] for token in tokens])
# 122 ?                ^^
# 112 -         text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)
# 112 ?                                                                       ----------------------------
# 123 +         text = bytearray([self.byte_decoder[c] for c in text]).decode(
# 124 +             "utf-8", errors=self.errors
# 125 +         )
# 128 +
# 116 -     with open(os.path.join(models_dir, 'encoder.json'), 'r') as f:
# 116 ?                                        ^            ^   ^ ^
# 130 +     with open(os.path.join(models_dir, "encoder.json"), "r") as f:
# 130 ?                                        ^            ^   ^ ^
# 118 -     with open(os.path.join(models_dir, 'vocab.bpe'), 'r', encoding="utf-8") as f:
# 118 ?                                        ^         ^   ^ ^
# 132 +     with open(os.path.join(models_dir, "vocab.bpe"), "r", encoding="utf-8") as f:
# 132 ?                                        ^         ^   ^ ^
# 120 -     bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]
# 120 ?                                                                            ^  ^
# 134 +     bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split("\n")[1:-1]]
# 134 ?                                                                            ^  ^
# 135 +     return Encoder(encoder=encoder, bpe_merges=bpe_merges,)
# 121 -     return Encoder(
# 122 -         encoder=encoder,
# 123 -         bpe_merges=bpe_merges,
# 124 -     )
# _codes/configuration.py -> ../codes/configuration.py
# 2 +
# 3 +
# 48 -         self.num_hidden_layers = self.n_layer
# 50 +         self.num_hidden_layers = self.n_layer
# 50 ?                                              +
# _codes/main.py -> ../codes/main.py
# 1 - from ast import arg
# 1 + from psutil import cpu_count
# 12 - import torch.nn.functional as F
# 15 +
# 21 + import wandb
# 21 -
# 22 - parser = argparse.ArgumentParser()
# 23 -
# 24 - parser.add_argument('--name', type=str, default="run",
# 25 -     help='Experiment name. Default: run')
# 26 - parser.add_argument('--model_config', type=str, default="./config.json",
# 27 -     help='Path to the configuration file. Default: ./config.json')
# 28 - parser.add_argument('--tokenizer_dir', type=str, default="./tokenizer",
# 29 -     help='Tokenizer file directory. Default: ./tokenizer')
# 30 - parser.add_argument('--num_epochs', type=int, default=20,
# 31 -     help='Number of training epoch. Default: 20')
# 32 - parser.add_argument('--cpu_count', type=int, default=20,
# 33 -     help='Number of CPU cores for evaluation. Default: 20')
# 34 - parser.add_argument('--batch_size', type=int, default=32,
# 35 -     help='The number of batch_size. Default: 32')
# 36 - parser.add_argument('--learning_rate', type=float, default=1e-4,
# 37 -     help='Learning rate during optimization. Default: 1e-4')
# 38 - parser.add_argument('--test', type=str, default=None,
# 39 -     help='Evaluate the model with the specified name. Default: None')
# 40 - parser.add_argument('--data_dir', type=str, default='./data',
# 41 -     help='Data directory. Default: ../data')
# 42 - parser.add_argument('--train_dir', type=str, default='./train_test',
# 43 -     help='Training directory for saving model. Default: ./train')
# 44 - parser.add_argument('--pretrain_dir', type=str, default='None',
# 45 -     help='Pre-Training directory for loading pretrained model. Default: None')
# 46 - parser.add_argument('--maxlen', type=int, default=35,
# 47 -     help='Maximum length for training/inference. Default: 35')
# 48 - parser.add_argument('--decode_strategy', type=str, choices=["random", "top-p", "top-k"], default="random",
# 49 -     help='The strategy for decoding. Can be "random", "top-p" or "top-k". Default: random')
# 50 - parser.add_argument('--temperature', type=float, default=1,
# 51 -     help='The temperature for decoding. Default: 1')
# 52 - parser.add_argument('--top_p', type=float, default=1.0,
# 53 -     help='The p for top-p sampling. Default: 1.0')
# 54 - parser.add_argument('--top_k', type=int, default=40,
# 55 -     help='The k for top-k sampling. Default: 40')
# 56 - args = parser.parse_args()
# 57 -
# 23 +
# 24 + #! 之后写 pipeline 的时候需要注意添加 test list
# 25 + #! test 的名字和  train 的 ckpt 完全一致。但是 wandb_run_name 会带上 temperature, top_k, top_p 等参数
# 26 + def parser_args():
# 27 +     parser = argparse.ArgumentParser()
# 28 +     parser.add_argument(
# 29 +         "--name", type=str, default=None, help="Experiment name. Used for wandb."
# 30 +     )
# 31 +     parser.add_argument(
# 32 +         "--model_config",
# 33 +         type=str,
# 34 +         default="config_base.json",
# 35 +         help="Path to the configuration file. Default: ./config.json",
# 36 +     )
# 37 +     parser.add_argument(
# 38 +         "--tokenizer_dir",
# 39 +         type=str,
# 40 +         default="./tokenizer",
# 41 +         help="Tokenizer file directory. Default: ./tokenizer",
# 42 +     )
# 43 +     parser.add_argument(
# 44 +         "--num_epochs",
# 45 +         type=int,
# 46 +         default=100,
# 47 +         help="Number of training epoch. Default: 20",
# 48 +     )
# 49 +     parser.add_argument(
# 50 +         "--cpu_count",
# 51 +         type=int,
# 52 +         default=20,
# 53 +         help="Number of CPU cores for evaluation. Default: 20",
# 54 +     )
# 55 +     parser.add_argument(
# 56 +         "--batch_size",
# 57 +         type=int,
# 58 +         default=128,
# 59 +         help="The number of batch_size. Default: 32",
# 60 +     )
# 61 +     parser.add_argument(
# 62 +         "--learning_rate",
# 63 +         type=float,
# 64 +         default=1e-4,
# 65 +         help="Learning rate during optimization. Default: 1e-4",
# 66 +     )
# 67 +     parser.add_argument(
# 68 +         "--test",
# 69 +         type=str,
# 70 +         default=None,
# 71 +         help="Evaluate the model with the specified name. Default: None",
# 72 +     )
# 73 +     parser.add_argument(
# 74 +         "--data_dir",
# 75 +         type=str,
# 76 +         default="./data",
# 77 +         help="Data directory. Default: ../data",
# 78 +     )
# 79 +     parser.add_argument(
# 80 +         "--train_dir",
# 81 +         type=str,
# 82 +         default="./train_ckpt",
# 83 +         help="Training directory for saving model. Default: ./train_ckpt",
# 84 +     )
# 85 +     #! 需要对比 pretrain 与 skcratch 的结果
# 86 +     parser.add_argument(
# 87 +         "--pretrain_dir",
# 88 +         type=str,
# 89 +         default=None,
# 90 +         help="Pre-Training directory for loading pretrained model. Default: None",
# 91 +     )
# 92 +     parser.add_argument(
# 93 +         "--max_len",
# 94 +         type=int,
# 95 +         default=35,
# 96 +         help="Maximum length for training/inference. Default: 35",
# 97 +     )
# 98 +     parser.add_argument(
# 99 +         "--decode_strategy",
# 100 +         type=str,
# 101 +         choices=["random", "top-p", "top-k"],
# 102 +         default="random",
# 103 +         help='The strategy for decoding. Can be "random", "top-p" or "top-k". Default: random',
# 104 +     )
# 105 +     parser.add_argument(
# 106 +         "--temperature",
# 107 +         type=float,
# 108 +         default=0.9,
# 109 +         help="The temperature for decoding. Default: 1",
# 110 +     )
# 111 +     parser.add_argument(
# 112 +         "--top_p",
# 113 +         type=float,
# 114 +         default=0.8,
# 115 +         help="The p for top-p sampling. Default: 0.8",
# 116 +     )
# 117 +     parser.add_argument(
# 118 +         "--top_k", type=int, default=40, help="The k for top-k sampling. Default: 40"
# 119 +     )
# 120 +     parser.add_argument(
# 121 +         "--using_wandb",
# 122 +         action="store_true",
# 123 +         help="Whether to use W&B logging. Default: False",
# 124 +     )
# 125 +     parser.add_argument(
# 126 +         "--waiting_epoch",
# 127 +         type=int,
# 128 +         default=3,
# 129 +         help="The epoch to start waiting for the tarining to end. Default: 5",
# 130 +     )
# 131 +     parser.add_argument(
# 132 +         "--num_layers",
# 133 +         type=int,
# 134 +         default=3,
# 135 +         help="The number of layers for the transformer. Default: 3",
# 136 +     )
# 137 +     parser.add_argument(
# 138 +         "--extract_layer",
# 139 +         type=int,
# 140 +         default=0,
# 141 +         help="The number of extract layers for the transformer. Default: 0",
# 142 +     )
# 143 +     parser.add_argument(
# 144 +         "--num_heads",
# 145 +         type=int,
# 146 +         default=12,
# 147 +         help="The number of heads for the transformer. Default: 12",
# 148 +     )
# 149 +     args = parser.parse_args()
# 150 +     return (
# 151 +         args,
# 152 +         args.name,
# 153 +         args.model_config,
# 154 +         args.tokenizer_dir,
# 155 +         args.num_epochs,
# 156 +         args.cpu_count,
# 157 +         args.batch_size,
# 158 +         args.learning_rate,
# 159 +         args.test,
# 160 +         args.data_dir,
# 161 +         args.train_dir,
# 162 +         args.pretrain_dir,
# 163 +         args.max_len,
# 164 +         args.decode_strategy,
# 165 +         args.temperature,
# 166 +         args.top_p,
# 167 +         args.top_k,
# 168 +         args.using_wandb,
# 169 +         args.waiting_epoch,
# 170 +         args.num_layers,
# 171 +         args.extract_layer,
# 172 +         args.num_heads,
# 173 +     )
# 174 +
# 175 +
# 177 +     return sentence_bleu(
# 60 -     return sentence_bleu(ele[0], ele[1], weights=ele[2], smoothing_function=SmoothingFunction().method1)
# 60 ?     ------ ^^^^^^^^^^^^^^                                                                              -
# 178 +         ele[0], ele[1], weights=ele[2], smoothing_function=SmoothingFunction().method1
# 178 ?      ^^^
# 179 +     )
# 180 +
# 70 -
# 209 +
# 212 +
# 213 +     # reference: https://superfastpython.com/multiprocessing-pool-python/
# 95 -         print("computing BLEU-%d"%ngrams)
# 219 +         print("computing BLEU-%d" % ngrams)
# 219 ?                                  + +
# 117 -         fw_bleu = (1.0 * sum(bleu_irl_fw) / len(bleu_irl_fw))
# 117 ?                   -                                         -
# 241 +         fw_bleu = 1.0 * sum(bleu_irl_fw) / len(bleu_irl_fw)
# 118 -         bw_bleu = (1.0 * sum(bleu_irl_bw) / len(bleu_irl_bw))
# 118 ?                   -                                         -
# 242 +         bw_bleu = 1.0 * sum(bleu_irl_bw) / len(bleu_irl_bw)
# 248 +         res.update(
# 249 +             {
# 124 -         res.update({"fw-bleu-%d"%ngrams : fw_bleu, \
# 124 ?         ^^^^^^^^^^^^                   -          --
# 250 +                 "fw-bleu-%d" % ngrams: fw_bleu,
# 250 ?         ^^^^^^^^            + +
# 125 -             "bw-bleu-%d"%ngrams : bw_bleu, \
# 125 ?                                -          --
# 251 +                 "bw-bleu-%d" % ngrams: bw_bleu,
# 251 ? ++++                        + +
# 126 -             "fw-bw-bleu-%d"%ngrams : fw_bw_bleu \
# 126 ?                                   -            ^^
# 252 +                 "fw-bw-bleu-%d" % ngrams: fw_bw_bleu,
# 252 ? ++++                           + +                  ^
# 253 +             }
# 127 -         })
# 127 ?         -
# 254 +         )
# 257 +
# 130 - def load_data(path, tokenizer, PAD_ID, field_list=["train", "dev", "test"], maxlen=40):
# 258 + def load_data(path, tokenizer, PAD_ID, field_list=["train", "dev", "test"], max_len=40):
# 258 ?                                                                                +
# 134 -         with open("%s/%s.txt"%(path, name)) as fin:
# 262 +         with open("%s/%s.txt" % (path, name)) as fin:
# 262 ?                              + +
# 137 -                 if len(tokens) < maxlen:
# 265 +                 if len(tokens) < max_len:
# 265 ?                                     +
# 266 +                     data[name].append(
# 138 -                     data[name].append([PAD_ID] + tokens + [PAD_ID]*(maxlen - len(tokens)))
# 138 ?                     ^^^^^^^^^^^^^^^^^^                                                   -
# 267 +                         [PAD_ID] + tokens + [PAD_ID] * (max_len - len(tokens))
# 267 ?                     ^^^^                            + +    +
# 268 +                     )
# 140 -                     data[name].append([PAD_ID] + tokens[:maxlen])
# 270 +                     data[name].append([PAD_ID] + tokens[:max_len])
# 270 ?                                                             +
# 273 +
# 289 +
# 292 +
# 160 - if __name__ == '__main__':
# 160 ?                ^        ^
# 293 + if __name__ == "__main__":
# 293 ?                ^        ^
# 161 -
# 162 -     print(args)
# 163 -     device = "cuda:6" #torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# 164 -     if not os.path.exists(args.train_dir):
# 165 -         os.mkdir(args.train_dir)
# 166 -     tokenizer = get_tokenizer(args.tokenizer_dir)
# 167 -     PAD_ID = tokenizer.encoder['<|endoftext|>']
# 168 -     print("Tokenizer PAD ID:", PAD_ID)
# 169 -
# 170 -     print("Loading Data ...")
# 171 -     data, data_remove_pad = load_data(path=args.data_dir, tokenizer=tokenizer, PAD_ID=PAD_ID, field_list=["train", "dev", "test"], maxlen=args.maxlen)
# 294 +     (
# 295 +         args,
# 296 +         name,
# 297 +         model_config,
# 298 +         tokenizer_dir,
# 299 +         num_epochs,
# 300 +         cpu_count,
# 301 +         batch_size,
# 302 +         learning_rate,
# 303 +         test,
# 304 +         data_dir,
# 305 +         train_dir,
# 306 +         pretrain_dir,
# 307 +         max_len,
# 308 +         decode_strategy,
# 309 +         temperature,
# 310 +         top_p,
# 311 +         top_k,
# 312 +         using_wandb,
# 313 +         waiting_epoch,
# 314 +         num_layers,
# 315 +         extract_layer,
# 316 +         num_heads,
# 317 +     ) = parser_args()
# 318 +     if extract_layer != 0:
# 319 +         extraction_dict = {1: "first", 2: "last", 3: "skip"}
# 320 +         wandb_run_name = f"extraction_{extraction_dict[extract_layer]}"
# 172 -     if args.test is None:
# 321 +     elif args.test is None:
# 321 ?     ++
# 324 +             with open(args.model_config) as fin:
# 325 +                 model_config = json.load(fin)
# 326 +                 print("layer num is " + str(args.num_layers))
# 327 +                 model_config["n_layer"] = args.num_layers
# 328 +                 model_config["n_head"] = args.num_heads
# 329 +                 config = ModelConfig(**model_config)
# 330 +                 if args.num_heads != 12:
# 331 +                      wandb_run_name = f"{model_config['n_layer']}_{args.batch_size}_{args.num_heads}"
# 332 +                 else:
# 333 +                     wandb_run_name = f"{model_config['n_layer']}_{args.batch_size}"
# 334 +         else:
# 335 +             #! 用 full 的话，batch_size = 64 会炸
# 336 +             try:
# 337 +                 if "\\" in pretrain_dir:
# 338 +                     wandb_run_name = str(pretrain_dir).split("\\")[-1][:-4]
# 339 +                 elif "/" in pretrain_dir:
# 340 +                     wandb_run_name = str(pretrain_dir).split("/")[-1][:-4]
# 341 +             except:
# 342 +                 wandb_run_name = pretrain_dir
# 343 +
# 344 +             wandb_run_name = wandb_run_name + f"_bs{args.batch_size}"
# 345 +     else:
# 346 +         try:
# 347 +             if "\\" in test:
# 348 +                 test_model = str(test).split("\\")[-1][:-4]
# 349 +             elif "/" in test:
# 350 +                 test_model = str(test).split("/")[-1][:-4]
# 351 +         except:
# 352 +             test_model = test
# 353 +         wandb_run_name = f"{test_model}_{decode_strategy}_{temperature}_{top_p}_{top_k}"
# 354 +
# 355 +     args.name = wandb_run_name
# 356 +     print(f"wandb name is {wandb_run_name}")
# 357 +
# 358 +     if using_wandb:
# 359 +         wandb.init(project="Transformer-Gen", entity="eren-zhao", name=wandb_run_name)
# 360 +         wandb.config = {
# 361 +             "name": args.name,
# 362 +             "model_config": model_config,
# 363 +             "tokenizer_dir": tokenizer_dir,
# 364 +             "num_epochs": num_epochs,
# 365 +             "cpu_count": cpu_count,
# 366 +             "batch_size": batch_size,
# 367 +             "learning_rate": learning_rate,
# 368 +             "test": test,
# 369 +             "data_dir": data_dir,
# 370 +             "train_dir": train_dir,
# 371 +             "pretrain_dir": pretrain_dir,
# 372 +             "max_len": max_len,
# 373 +             "decode_strategy": decode_strategy,
# 374 +             "temperature": temperature,
# 375 +             "top_p": top_p,
# 376 +             "top_k": top_k,
# 377 +         }
# 378 +
# 379 +     device = "cuda" if torch.cuda.is_available() else "cpu"
# 380 +
# 381 +     if not os.path.exists(args.train_dir):
# 382 +         os.makedirs(args.train_dir)
# 383 +
# 384 +     tokenizer = get_tokenizer(args.tokenizer_dir)
# 385 +     PAD_ID = tokenizer.encoder["<|endoftext|>"]
# 386 +     print("Tokenizer PAD ID:", PAD_ID)
# 387 +
# 388 +     print("Loading Data ...")
# 389 +     data, data_remove_pad = load_data(
# 390 +         path=args.data_dir,
# 391 +         tokenizer=tokenizer,
# 392 +         PAD_ID=PAD_ID,
# 393 +         field_list=["train", "dev", "test"],
# 394 +         max_len=args.max_len,
# 395 +     )
# 396 +
# 397 +     if args.test is None:
# 398 +         if args.extract_layer != 0:
# 399 +             print("Loading Model from the extracted layers of full model")
# 406 +             state_dict = model.state_dict()
# 407 +             full_model = torch.load(args.pretrain_dir)
# 408 +             ckpt = full_model.state_dict()
# 409 +             mappings = [
# 410 +                 {},
# 411 +                 {"0": "0", "1": "1", "2": "2"},
# 412 +                 {"0": "9", "1": "10", "2": "11"},
# 413 +                 {"0": "0", "1": "5", "2": "11"},
# 414 +             ]
# 415 +             mapping = mappings[args.extract_layer]
# 416 +             for key in state_dict.keys():
# 417 +                 # ！ TODO 这里的 key 是什么
# 418 +                 if key.startswith("transformer.h"):
# 419 +                     name = key.split(".")
# 420 +                     name[2] = mapping[name[2]]
# 421 +                     name = ".".join(name)
# 422 +                 else:
# 423 +                     name = key
# 424 +
# 425 +                 state_dict[key] = ckpt[name]
# 426 +
# 427 +             model.load_state_dict(state_dict)
# 428 +
# 429 +         elif args.pretrain_dir is None:
# 430 +             model = TfmrLMHeadModel(config)
# 431 +             init_weights_func = get_init_weights_func(config=config)
# 432 +             model.apply(init_weights_func)
# 434 +             if os.path.exists(args.pretrain_dir):
# 182 -             model_path = os.path.join(args.pretrain_dir, 'pretrained_ckpt.tar')
# 183 -             if os.path.exists(model_path):
# 184 -                 print("Loading model from %s" % model_path)
# 184 ?                                                 ^^ ^^^^^^^
# 435 +                 print("Loading model from %s" % args.pretrain_dir)
# 435 ?                                                 ^^^^^^^^^^^^^^ ^^
# 185 -                 model = torch.load(model_path)
# 185 ?                                    ^^ ^^^^^^^
# 436 +                 model = torch.load(args.pretrain_dir)
# 436 ?                                    ^^^^^^^^^^^^^^ ^^
# 187 -                 raise RuntimeError("No such checkpoint: %s"%model_path)
# 187 ?                                                             ^^ ^^^^^^^
# 438 +                 raise RuntimeError("No such checkpoint: %s" % args.pretrain_dir)
# 438 ?                                                            + ^^^^^^^^^^^^^^^ ^^
# 189 -         print(model)
# 440 +         if using_wandb:
# 441 +             wandb.watch(model)
# 443 +         optimizer = optim.Adam(
# 191 -
# 192 -
# 193 -
# 194 -         optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=0)
# 194 ?         --------- - ^^^^^^^^^^^                                                         -
# 444 +             model.parameters(), lr=args.learning_rate, weight_decay=0
# 444 ?           ^^
# 445 +         )
# 448 +         waiting_epoch = 0
# 205 -                 st, ed = ed, (ed + args.batch_size) if (ed + args.batch_size < len(data["train"])) else len(data["train"])
# 457 +                 st, ed = (
# 458 +                     ed,
# 459 +                     (ed + args.batch_size)
# 460 +                     if (ed + args.batch_size < len(data["train"]))
# 461 +                     else len(data["train"]),
# 462 +                 )
# 466 +                 loss = model(
# 209 -                 loss = model(input_ids=batched_data, labels=batched_data, PAD_ID=PAD_ID)["loss"]
# 209 ?                 ---- - ^^^^^^                                                          ---------
# 467 +                     input_ids=batched_data, labels=batched_data, PAD_ID=PAD_ID
# 467 ?                   ^^
# 468 +                 )["loss"]
# 214 -                 if (batch_num) % 10 == 0:
# 214 ?                    -         -
# 473 +                 if batch_num % 10 == 0:
# 215 -                     print("Epoch %d Batch %d, train loss %f" % (epoch, batch_num, np.mean(losses[-100:])))
# 474 +                     taring_loss = np.mean(losses[-100:])
# 475 +                     if using_wandb:
# 476 +                         wandb.log(
# 477 +                             {
# 478 +                                 "training_epoch": epoch,
# 479 +                                 "batch_num": batch_num,
# 480 +                                 "train_loss_batch": taring_loss,
# 481 +                             }
# 482 +                         )
# 483 +                     print(
# 484 +                         "Epoch %d Batch %d, train loss %f"
# 485 +                         % (epoch, batch_num, taring_loss)
# 486 +                     )
# 219 -             val_loss, val_ppl = fast_evaluate(model=model, data=data["dev"], batch_size=args.batch_size, PAD_ID=PAD_ID, device=device)
# 490 +             val_loss, val_ppl = fast_evaluate(
# 491 +                 model=model,
# 492 +                 data=data["dev"],
# 493 +                 batch_size=args.batch_size,
# 494 +                 PAD_ID=PAD_ID,
# 495 +                 device=device,
# 496 +             )
# 501 +                 os.makedirs(args.train_dir, exist_ok=True)
# 502 +                 with open(
# 224 -                 with open(os.path.join(args.train_dir, 'checkpoint_%s.pth.tar' % args.name), 'wb') as fout:
# 224 ?                 ---- ^^^^^                             ^^^^^^^^^^^^  ----    ^               ^  ^^^^^^^^^^^
# 503 +                     os.path.join(args.train_dir, "%s.tar" % args.name), "wb"
# 503 ?                  ^^^                             ^      ^               ^  ^
# 504 +                 ) as f:
# 225 -                     torch.save(model, fout)
# 225 ?                                        ---
# 505 +                     torch.save(model, f)
# 226 -
# 228 -                 print("Epoch " + str(epoch) + " of " + str(args.num_epochs) + " took " + str(epoch_time) + "s")
# 507 +                 print(
# 508 +                     "Epoch "
# 509 +                     + str(epoch)
# 510 +                     + " of "
# 511 +                     + str(args.num_epochs)
# 512 +                     + " took "
# 513 +                     + str(epoch_time)
# 514 +                     + "s"
# 515 +                 )
# 521 +                 if using_wandb:
# 522 +                     print("logging to wandb")
# 523 +                     wandb.log(
# 524 +                         {
# 525 +                             "epoch": epoch,
# 526 +                             "train_loss": train_loss,
# 527 +                             "val_loss": val_loss,
# 528 +                             "val_ppl": val_ppl,
# 529 +                             "best_epoch": best_epoch,
# 530 +                             "best_val_ppl": best_val_ppl,
# 531 +                         }
# 532 +                     )
# 533 +                 waiting_epoch = 0
# 535 +                 if using_wandb:
# 536 +                     wandb.log(
# 537 +                         {
# 538 +                             "epoch": epoch,
# 539 +                             "train_loss": train_loss,
# 540 +                             "val_loss": val_loss,
# 541 +                             "val_ppl": val_ppl,
# 542 +                         }
# 543 +                     )
# 235 -                 print("Validation loss: %.3f, becomes larger. Stop training."%val_ppl)
# 544 +                 print("Validation loss: %.3f, becomes larger. Stop training." % val_ppl)
# 544 ?                                                                              + +
# 545 +                 waiting_epoch += 1
# 546 +                 print()
# 547 +                 print("waiting_epoch is " + str(waiting_epoch))
# 548 +                 if waiting_epoch >= args.waiting_epoch:
# 236 -                 break
# 549 +                     break
# 549 ? ++++
# 239 -         model_path = os.path.join(args.train_dir, 'checkpoint_%s.pth.tar' % args.test)
# 552 +         #! test 直接是 path
# 240 -         if os.path.exists(model_path):
# 240 ?                           ^^^ ^^^^ -
# 553 +         if os.path.exists(args.test):
# 553 ?                           ^^^^^^ ^
# 241 -             print("Loading model from %s" % model_path)
# 241 ?                                             ^^^ ^^^^ -
# 554 +             print("Loading model from %s" % args.test)
# 554 ?                                             ^^^^^^ ^
# 242 -             model = torch.load(model_path)
# 242 ?                                ^^^ ^^^^ -
# 555 +             model = torch.load(args.test)
# 555 ?                                ^^^^^^ ^
# 246 -         print(model)
# 247 -         test_loss, test_ppl = fast_evaluate(model=model, data=data["test"], batch_size=args.batch_size, PAD_ID=PAD_ID, device=device)
# 559 +         if using_wandb:
# 560 +             wandb.watch(model)
# 561 +         print("Start testing.")
# 562 +
# 563 +         print("testing batch_size is: " + str(args.batch_size))
# 564 +         test_loss, test_ppl = fast_evaluate(
# 565 +             model=model,
# 566 +             data=data["test"],
# 567 +             batch_size=args.batch_size,
# 568 +             PAD_ID=PAD_ID,
# 569 +             device=device,
# 570 +         )
# 249 -         result = model.inference(device=device, PAD_ID=PAD_ID,
# 250 -             batch_size=args.batch_size, maxlen=args.maxlen, decode_strategy=args.decode_strategy, temperature=args.temperature, top_p=args.top_p, top_k=args.top_k)
# 251 -         with open('output_%s.txt'%args.decode_strategy, 'w') as fout:
# 572 +         result = model.inference(
# 573 +             device=device,
# 574 +             PAD_ID=PAD_ID,
# 575 +             batch_size=args.batch_size,
# 576 +             max_len=args.max_len,
# 577 +             decode_strategy=args.decode_strategy,
# 578 +             temperature=args.temperature,
# 579 +             top_p=args.top_p,
# 580 +             top_k=args.top_k,
# 581 +         )
# 582 +         os.makedirs("./test_results", exist_ok=True)
# 583 +         with open("./test_results/%s.txt" % args.name, "a+") as f:
# 254 -                 print(k, out)
# 255 -                 fout.write(out + "\n")
# 255 ?                  ---
# 586 +                 f.write(out + "\n")
# 587 +             f.write("----------------------------------------------------\n")
# 257 -         print("        test_set, forward BLEU-4 %.3f, backward BLEU-4 %.3f, harmonic BLEU-4 %.3f" % (eval_result["fw-bleu-4"], eval_result["bw-bleu-4"], eval_result["fw-bw-bleu-4"]))
# 258 -         print("        test_set, write inference results to output_%s.txt"%args.decode_strategy)
# 589 +         if using_wandb:
# 590 +             wandb.log(
# 591 +                 {
# 592 +                     "test_loss": test_loss,
# 593 +                     "test_ppl": test_ppl,
# 594 +                     "eval_result": eval_result,
# 595 +                     "fw-bleu-4": eval_result["fw-bleu-4"],
# 596 +                     "bw-bleu-4": eval_result["bw-bleu-4"],
# 597 +                     "fw-bw-bleu-4": eval_result["fw-bw-bleu-4"],
# 598 +                 }
# 599 +             )
# 600 +         print(
# 601 +             "        test_set, forward BLEU-4 %.3f, backward BLEU-4 %.3f, harmonic BLEU-4 %.3f"
# 602 +             % (
# 603 +                 eval_result["fw-bleu-4"],
# 604 +                 eval_result["bw-bleu-4"],
# 605 +                 eval_result["fw-bw-bleu-4"],
# 606 +             )
# 607 +         )

